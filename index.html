<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Potsawee (Punpun) Manakul</title>
    <meta name="description" content="Potsawee (Punpun) Manakul - Research Scientist working on Speech & Multimodal AI, Audio Large Language Models, and foundational speech models.">
    <!-- Load Barlow font to exactly match Philip's site -->
    <link href="https://fonts.googleapis.com/css2?family=Barlow:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    <!-- FontAwesome to display modern circular icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="css/main.css">
    <!-- Favicon -->
    <link rel="icon" type="image/svg+xml" href="images/favicon.svg">
</head>
<body>
    <main class="container">
        
        <!-- Header Section -->
        <div class="header-section">
            <div class="profile-img-container">
                <img class="profile-picture" src="images/potsawee.jpg" alt="Potsawee (Punpun) Manakul">
            </div>
            <div class="header-info">
                <h1>Potsawee (Punpun) Manakul</h1>
                <div class="header-subtitle">
                    Research Scientist ¬∑ Speech & Multimodal AI
                </div>
                <div class="icon-row">
                    <a href="#" class="icon-link" aria-label="CV" target="_blank">
                        <span class="icon-text">CV</span>
                    </a>
                    <a href="https://scholar.google.com/citations?hl=en&user=dVgn6boAAAAJ" class="icon-link" aria-label="Google Scholar" target="_blank">
                        <i class="fa-solid fa-graduation-cap"></i>
                    </a>
                    <a href="https://github.com/potsawee" class="icon-link" aria-label="GitHub" target="_blank">
                        <i class="fa-brands fa-github"></i>
                    </a>
                    <a href="https://huggingface.co/potsawee" class="icon-link" title="HuggingFace" target="_blank">
                        <!-- Use official SVG logo from SODA project -->
                        <img src="images/hf-logo-bw.svg" style="width: 21.5px;">
                    </a>
                    <a href="https://x.com/potsawee_m" class="icon-link" aria-label="X / Twitter" target="_blank">
                        <i class="fa-brands fa-x-twitter"></i>
                    </a>
                    <a href="mailto:potsawee@stanford.edu" class="icon-link" aria-label="Email" target="_blank">
                        <i class="fa-solid fa-envelope"></i>
                    </a>
                </div>
            </div>
        </div>

        <hr>

        <!-- About Me Section -->
        <section class="content-section" id="about">
            <h2 class="section-title">About Me</h2>
            <p>I'm currently a Visiting Researcher at the <a href="https://nlp.stanford.edu/" target="_blank">Stanford NLP Group</a>, working with Prof. <a href="https://cs.stanford.edu/~diyiy/" target="_blank">Diyi Yang</a>. My research focuses on audio/speech LLMs, including training foundation models and studying their scaling properties (<a href="https://soda-audio.github.io/" target="_blank">Project SODA</a>). I also work on multimodal AI safety and audio modeling for low-resource languages.</p>
            
            <p>From Nov 2023 to Jan 2026, I was a founding team member &amp; first AI researcher at <a href="https://opentyphoon.ai/" target="_blank">Typhoon (SCB 10X)</a>, an AI lab advancing Thai AI models (e.g., LLM, ASR, TTS) where I led the development of audio/speech models.</p>

            <p>I completed my PhD in the Speech Group at Cambridge in May 2024, advised by Prof. <a href="https://scholar.google.com/citations?user=RSFlmjIAAAAJ" target="_blank">Mark Gales</a>. My research focused primarily on NLP‚Äîparticularly text summarization and LLM factuality. Notable work includes <a href="https://github.com/potsawee/selfcheckgpt" target="_blank">SelfCheckGPT</a> (1400+ citations &amp; 600+ GitHub stars). During my PhD, I also had a research internship at Amazon. Before this, I earned my BA and MEng degrees, also from Cambridge.</p>
        </section>

        <hr>

        <!-- Research Themes Section -->
        <section class="content-section" id="themes">
            <h2 class="section-title">Research Themes</h2>
            <ul class="theme-list">
                <li><span class="theme-label">Multimodal, Audio, Speech:</span> <a href="https://soda-audio.github.io" target="_blank">SODA</a>, <a href="https://arxiv.org/abs/2507.12705" target="_blank">AudioJudge</a>, <a href="https://talkarena.org/" target="_blank">TalkArena</a>, <a href="https://arxiv.org/abs/2409.10999" target="_blank">Low-Resource AudioLLM</a>, <a href="https://arxiv.org/abs/2510.15231" target="_blank">Extended Context AudioLLM</a>, <a href="https://arxiv.org/abs/2307.04172" target="_blank">ASR Error Correction with LLM</a></li>
                <li><span class="theme-label"><a href="https://opentyphoon.ai/" target="_blank">Typhoon, Thai AI</a>:</span> <a href="https://opentyphoon.ai/blog/en/typhoon-2-multimodal-release-research-preview-200fe9015ad9" target="_blank">Typhoon1/2-Audio</a>, Typhoon-TTS (internal), <a href="https://opentyphoon.ai/blog/en/typhoon-asr-realtime-release" target="_blank">Typhoon-ASR</a>, <a href="https://opentyphoon.ai/blog/en/introducing-typhoon-2-api-pro-accessible-production-grade-thai-llms-3e139c077aab" target="_blank">Typhoon1/2-LLM</a>, <a href="https://crfm.stanford.edu/2024/09/04/thaiexam.html" target="_blank">Thai-HELM</a></li>
                <li><span class="theme-label">NLG Eval, Factuality, AI Safety:</span> <a href="https://github.com/potsawee/selfcheckgpt" target="_blank">SelfCheckGPT</a>, <a href="https://arxiv.org/abs/2405.13684" target="_blank">CrossCheckGPT</a>, <a href="https://arxiv.org/abs/2301.12307" target="_blank">MQAG</a>, <a href="https://arxiv.org/abs/2307.07889" target="_blank">LLM Comparative Assessment</a>, <a href="https://arxiv.org/abs/2410.10215" target="_blank">SkillAggregation</a>, <a href="https://arxiv.org/abs/2505.02884" target="_blank">LLM Unlearning</a></li>
            </ul>
        </section>

        <hr>

        <!-- News Section -->
        <section class="content-section" id="news">
            <h2 class="section-title">News</h2>
            <ul class="news-list">
                <li><span class="news-date">[2026/Feb]</span> We released <a href="https://soda-audio.github.io/" target="_blank">SODA (Scaling Open Discrete Audio)</a></li>
                <li><span class="news-date">[2026/Feb]</span> Transitioned from Typhoon to work as an independent researcher on Thai speech models</li>
                <li><span class="news-date">[2026/Jan]</span> <a href="https://arxiv.org/abs/2507.12705" target="_blank">AudioJudge</a> and <a href="https://arxiv.org/abs/2510.15231" target="_blank">Extended Context AudioLLM</a> accepted to EACL 2026 (as first-author &amp; mentor)</li>
                <li><span class="news-date">[2025/Aug]</span> <a href="https://arxiv.org/abs/2505.02884" target="_blank">LLM Unlearning</a> and <a href="https://arxiv.org/abs/2505.14157" target="_blank">Prompt Engineering for RFT</a> accepted at EMNLP 2025 (as a co-author)</li>
                <li><span class="news-date">[2025/Jul]</span> Started a Visiting Scholar position at <a href="https://nlp.stanford.edu/" target="_blank">Stanford NLP</a> (hosted by Prof. Diyi Yang)</li>
                <li><span class="news-date">[2025/May]</span> <a href="https://arxiv.org/abs/2409.10999" target="_blank">Typhoon-Audio</a> accepted at Interspeech 2025 (as the first author)</li>
                <li><span class="news-date">[2025/May]</span> <a href="https://arxiv.org/abs/2410.10215" target="_blank">SkillAggregation</a> and <a href="https://talkarena.org/" target="_blank">TalkArena</a> accepted at ACL 2025 (as a co-author)</li>
                <li><span class="news-date">[2025/Feb]</span> We released Typhoon reasoning models: <a href="https://arxiv.org/abs/2502.09042" target="_blank">Typhoon-T1</a> &amp; <a href="https://arxiv.org/abs/2502.09056" target="_blank">Typhoon-R1</a></li>
                <li><span class="news-date">[2024/Dec]</span> We released <a href="https://arxiv.org/abs/2412.13702" target="_blank">Typhoon 2</a> text and multimodal models</li>
                <li><span class="news-date">[2024/Aug]</span> Invited talk at the <a href="https://www.sec.gov/" target="_blank">U.S. SEC</a> on LLM Hallucination</li>
                <li><span class="news-date">[2024/Aug]</span> Invited talk at the <a href="https://connect.aisingapore.org/2024/09/breaking-barriers-building-bridges-increasing-language-representation-in-southeast-asia/" target="_blank">3rd SEA Language Summit</a> on Multimodal LLMs</li>
                <li><span class="news-date">[2024/May]</span> Completed Ph.D. viva and left Cambridge, UK</li>
            </ul>
        </section>

        <hr>

        <!-- Publications Section -->
        <section class="content-section" id="publications">
            <h2 class="section-title">Selected Projects &amp; Publications</h2>
            
            <div class="publication-item">
                <a href="https://soda-audio.github.io/" class="pub-title" target="_blank">Scaling Open Discrete Audio (SODA)</a>
                <span class="pub-authors"><u>Potsawee Manakul</u>, Woody Haosheng Gan, Martijn Bartelds, Guangzhi Sun, William Held, Diyi Yang</span>
                <span class="pub-venue">Preprint Feb 2026</span>
                <div class="pub-links">
                    <a href="https://soda-audio.github.io/" target="_blank"><span class="paper-emoji">üè†</span> Project Page</a>
                    <span style="margin: 0 6px;">|</span>
                    <a href="https://arxiv.org/abs/2602.16687" target="_blank"><span class="paper-emoji">üìÑ</span> Paper</a>
                    <span style="margin: 0 6px;">|</span>
                    <a href="https://soda-audio.github.io/experiment_log.html" target="_blank"><span class="paper-emoji">üìà</span> Experiment Log</a>
                    <span style="margin: 0 6px;">|</span>
                    <a href="https://github.com/potsawee/marin/tree/audio-release-pr/experiments/audio" target="_blank"><i class="fa-brands fa-github paper-emoji"></i> Code</a>
                    <span style="margin: 0 6px;">|</span>
                    <a href="https://huggingface.co/soda-research" target="_blank"><span class="paper-emoji">ü§ó</span> Models &amp; Data</a>
                    <span style="margin: 0 6px;">|</span>
                    <a href="https://huggingface.co/spaces/potsawee/soda-demo" target="_blank"><span class="paper-emoji">üéÆ</span> Demo</a>
                </div>
            </div>

            <div class="publication-item">
                <a href="https://arxiv.org/abs/2507.12705" class="pub-title" target="_blank">AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation</a>
                <span class="pub-authors"><u>Potsawee Manakul*</u>, Woody Haosheng Gan*, Michael J. Ryan, Ali Sartaz Khan, Warit Sirichotedumrong, Kunat Pipatanakul, William Held, Diyi Yang</span>
                <span class="pub-venue">EACL 2026</span>
                <div class="pub-links">
                    <a href="https://arxiv.org/abs/2507.12705" target="_blank"><span class="paper-emoji">üìÑ</span> Paper</a>
                    <span style="margin: 0 6px;">|</span>
                    <a href="https://github.com/Woodygan/AudioJudge" target="_blank"><i class="fa-brands fa-github paper-emoji"></i> Code</a>
                </div>
            </div>

            <div class="publication-item">
                <a href="https://opentyphoon.ai/model/typhoon-2-audio" class="pub-title" target="_blank">Enhancing Low-Resource Language and Instruction Following Capabilities of Audio Language Models</a>
                <span class="pub-authors"><u>Potsawee Manakul</u>, Guangzhi Sun, Warit Sirichotedumrong, Kasima Tharnpipitchai, Kunat Pipatanakul</span>
                <span class="pub-venue">Interspeech 2025</span>
                <div class="pub-links">
                    <a href="https://arxiv.org/abs/2409.10999" target="_blank"><span class="paper-emoji">üìÑ</span> Paper</a>
                    <span style="margin: 0 6px;">|</span>
                    <a href="https://github.com/scb-10x/typhoon2-audio/" target="_blank"><i class="fa-brands fa-github paper-emoji"></i> Code</a>
                    <span style="margin: 0 6px;">|</span>
                    <a href="https://huggingface.co/collections/typhoon-ai/typhoon-2-multimodal" target="_blank"><span class="paper-emoji">ü§ó</span> Model</a>
                    <!-- <span style="margin: 0 6px;">|</span> -->
                    <!-- <a href="https://opentyphoon.ai/blog/en/typhoon-2-multimodal-release-research-preview-200fe9015ad9" target="_blank"><span class="paper-emoji">üìù</span> Blog</a> -->
                </div>
            </div>

            <div class="publication-item">
                <a href="https://github.com/potsawee/selfcheckgpt" class="pub-title" target="_blank">SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models</a>
                <span class="pub-authors"><u>Potsawee Manakul</u>, Adian Liusie, Mark Gales</span>
                <span class="pub-venue">EMNLP 2023</span>
                <div class="pub-links">
                    <a href="https://github.com/potsawee/selfcheckgpt" target="_blank"><span class="paper-emoji">üè†</span> <strong>Project Page &amp; Code (600+ GitHub stars)</strong></a>
                    <span style="margin: 0 6px;">|</span>
                    <a href="https://pypi.org/project/selfcheckgpt/" target="_blank"><i class="fa-brands fa-python paper-emoji"></i> PyPI</a>
                    <span style="margin: 0 2px;"></span>
                    <a href="https://pepy.tech/project/selfcheckgpt" target="_blank"><img src="https://pepy.tech/badge/selfcheckgpt" alt="Downloads" style="vertical-align: middle; height: 18px; margin-top: -3px; filter: grayscale(100%) opacity(90%);"></a>
                </div>
                <div class="pub-annotations">
                    <div><a href="https://wapo.st/3IMgiZw" target="_blank">‚Üí Featured in the Washington Post about AI Hallucination (May 2023)</a></div>
                    <div><a href="https://www.deeplearning.ai/short-courses/quality-safety-llm-applications" target="_blank">‚Üí Featured in DeepLearning.AI course on LLM Safety</a></div>
                    <div><a href="https://web.stanford.edu/class/cs329t/syllabus.html" target="_blank">‚Üí Featured in reading list of Stanford CS 329T (Fall 2023)</a></div>
                    <div><a href="https://www.sec.gov/" target="_blank">‚Üí Invited talk at AI CoP Seminar @ U.S. SEC on LLM Hallucination (Aug 2024)</a></div>
                    <div>‚Üí 1400+ citations</div>
                </div>
            </div>

            <div class="publication-item">
                <a href="https://github.com/potsawee/mqag0" class="pub-title" target="_blank">MQAG: Multiple-choice Question Answering and Generation for Assessing Information Consistency</a>
                <span class="pub-authors"><u>Potsawee Manakul</u>, Adian Liusie, Mark Gales</span>
                <span class="pub-venue">AACL 2023 <strong>(Area Chair Paper Award)</strong></span>
                <div class="pub-links">
                    <a href="https://github.com/potsawee/mqag0" target="_blank"><i class="fa-brands fa-github paper-emoji"></i> Code</a>
                    <span style="margin: 0 6px;">|</span>
                    <a href="https://huggingface.co/collections/potsawee/mqag-models" target="_blank"><span class="paper-emoji">ü§ó</span> Models</a>
                </div>
                
            </div>

            <div class="publication-item">
                <a href="https://aclanthology.org/2021.acl-long.470/" class="pub-title" target="_blank">Long-Span Summarization via Local Attention and Content Selection</a>
                <span class="pub-authors"><u>Potsawee Manakul</u>, Mark Gales</span>
                <span class="pub-venue">ACL 2021</span>
                <div class="pub-links">
                    <a href="https://github.com/potsawee/longsum0" target="_blank"><i class="fa-brands fa-github paper-emoji"></i> Code & Models</a>
                </div>
                <div class="pub-annotations">
                    <div><a href="https://podcastsdataset.byspotify.com/" target="_blank">‚Üí System winning 1st place in the Spotify Podcast Summarisation task 2020</a></div>
                </div>
            </div>

            <div class="publication-item">
                <a href="https://www.isca-speech.org/archive/interspeech_2020/manakul20_interspeech.html" class="pub-title" target="_blank">Abstractive Spoken Document Summarization using Hierarchical Model with Multi-stage Attention Diversity</a>
                <span class="pub-authors"><u>Potsawee Manakul</u>, Mark J. F. Gales, Linlin Wang</span>
                <span class="pub-venue">Interspeech 2020 <strong>(Best Student Paper Award Finalist)</strong></span>
                <div class="pub-links">
                    <a href="https://github.com/potsawee/spoken_summ_div" target="_blank"><i class="fa-brands fa-github paper-emoji"></i> Code</a>
                </div>
            </div>

            <div style="margin-top: 15px;">
                <a href="https://scholar.google.com/citations?user=dVgn6boAAAAJ" style="font-size: 15px; color: #4b5563; font-weight: 600;" target="_blank">‚Üí View full list on Google Scholar</a>
            </div>
        </section>

        <hr>

        <!-- Honours &amp; Awards Section -->
        <section class="content-section" id="honours">
            <h2 class="section-title">Honours and Awards</h2>
            <ul class="news-list">
                <li>AACL 2023 Area Chair Paper Award</li>
                <li>Interspeech 2020 Best Student Paper Finalist</li>
                <li>Cambridge International &amp; St John's College Scholarship (2019)</li>
                <li>BP 1st Year Prize: top 1% in 1st-year Cambridge Engineering (2016)</li>
                <li>Thai Government Scholarship (2015)</li>
                <li>ü•à Silver Medalist at <a href="https://en.wikipedia.org/wiki/International_Physics_Olympiad" target="_blank">IPhO</a> 2014; ü•á Gold Medalist at <a href="https://en.wikipedia.org/wiki/Asian_Physics_Olympiad" target="_blank">APhO</a> 2014</li>
            </ul>
        </section>

        <hr>

        <!-- Service Section -->
        <section class="content-section" id="service">
            <h2 class="section-title">Service</h2>
            <ul class="news-list" style="columns: 2; -webkit-columns: 2; -moz-columns: 2; column-gap: 40px;">
                <li><strong>Area Chair:</strong> ACL 2025</li>
                <li><strong>ACL Rolling Review:</strong> 2024, 2025, 2026</li>
                <li><strong>NeurIPS:</strong> 2023, 2024, 2025</li>
                <li><strong>ICML:</strong> 2023, 2024, 2025, 2026</li>
                <li><strong>ICLR:</strong> 2024, 2025</li>
                <li><strong>Interspeech:</strong> 2023, 2024, 2025</li>
                <li><strong>COLM:</strong> 2024, 2025</li>
                <li><strong>AAAI:</strong> 2025</li>
                <li><strong>IEEE TASLP:</strong> 2022, 2023, 2024, 2025</li>
            </ul>
        </section>

        <!-- Tracker (From original) -->
        <div style="text-align: center; margin-top: 60px;">
            <a href='https://clustrmaps.com/site/1bjxf' title='Visit tracker'><img style="height: 0px;width: 0px;" src='https://clustrmaps.com/map_v2.png?cl=bdbdbd&w=180&t=n&d=H9-CpJ0JXgcgvjxD8q_jUzgZnCCW1mLaWsJuJYPoQnE&co=ffffff&ct=333333' alt="Tracker"/></a>
        </div>

    </main>
</body>
</html>